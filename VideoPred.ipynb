{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyh44r0vh2eW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f2b66a-a399-4ca7-fc35-abcdfd3d58c8"
      },
      "source": [
        "!pip install kornia\n",
        "!pip install pafy youtube_dl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kornia\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/b2/8a968f1d7fb1d651a77c1ad7ffce9fc7b4dbd250eecaa9e2f21714fcfb2e/kornia-0.5.0-py2.py3-none-any.whl (271kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 133kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 163kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 174kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 184kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 194kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 204kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 215kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 225kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 235kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 245kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 256kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 266kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kornia) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->kornia) (3.7.4.3)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.5.0\n",
            "Collecting pafy\n",
            "  Downloading https://files.pythonhosted.org/packages/74/69/829919eeadff695338f98fa12bb99e45490761a2010c8d688d88b6df194a/pafy-0.5.5-py2.py3-none-any.whl\n",
            "Collecting youtube_dl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/cb/829848d3f15e63e51f1a1c874c2151690c2365dd95d7507c995531a28efb/youtube_dl-2021.4.7-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pafy, youtube-dl\n",
            "Successfully installed pafy-0.5.5 youtube-dl-2021.4.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npognjYsgoM9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils import model_zoo\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy as np\n",
        "from math import exp, ceil\n",
        "import os\n",
        "import os.path\n",
        "import gzip\n",
        "from six.moves import urllib\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import pafy\n",
        "import cv2 as cv\n",
        "import random\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "from kornia.filters import get_gaussian_kernel2d, filter2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09eXSTxBWk9U"
      },
      "source": [
        "def seed_all(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1By1BEKB6m3"
      },
      "source": [
        "MOUNT_GDRIVE = True # Use GoogleDrive or the current dir as working dir\n",
        "TRAIN_MODE = False # Train or Test mode\n",
        "TEST_EPOCH = 160 # Use saved model at given epoch for testing\n",
        "\n",
        "PROG_IMG_RESIZING = True # Progressive image resizing mode for training\n",
        "TRAIN_STEP = 1 # Step: 1, 2, 3, [4]\n",
        "\n",
        "RANDOMIZE = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZWVublBkzsR"
      },
      "source": [
        "config = {\n",
        "    'learning_rate' : 1e-4,\n",
        "    'weight_decay' : 1e-5,\n",
        "    'optimizer': optim.Adam,\n",
        "    'dataset_name' : 'Robot', # 'Mnist' or 'Robot'\n",
        "    'n_frames' : 3, # number of input/target frames\n",
        "    'mse_gain' : 0.3, # mse_loss coefficient in range [0, 1]\n",
        "    'ssim_win_size' : 11,\n",
        "    'n_epochs' : 100, # number of epochs to train in a single train step\n",
        "    'save_step' : 10, # save the model after each speacified epochs\n",
        "    'start_epoch': 1, # epoch up to which checkpoint model has been trained\n",
        "    'loc_aware': True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q67qDhfR4B4x"
      },
      "source": [
        "def init_config():\n",
        "  if config['dataset_name'] == 'Mnist':\n",
        "    config['in_size'] = (64, 64)\n",
        "    config['batch_size'] = 64\n",
        "    \n",
        "    config['n_workers'] = 2\n",
        "    config['train_split'] = 9000\n",
        "    \n",
        "    config['mse_gain'] = 0.35\n",
        "  else:\n",
        "    assert config['dataset_name'] == 'Robot'\n",
        "    config['in_size'] = (224, 320)\n",
        "    config['batch_size'] = 4\n",
        "    \n",
        "    config['n_workers'] = 0\n",
        "    config['train_split'] = None\n",
        "\n",
        "    config['mse_gain'] = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvMP502zefBI"
      },
      "source": [
        "def adjust_config(train_step):\n",
        "  x = 32\n",
        "  if config['dataset_name'] == 'Mnist':\n",
        "    config['n_epochs_list'] = [50, 50, 70]\n",
        "    in_size_dict = {\n",
        "        1: (x*2, x*2),\n",
        "        2: (x*4, x*4),\n",
        "        3: (x*7, x*7)\n",
        "    }\n",
        "    bs_dict = {\n",
        "        1: 64,\n",
        "        2: 16,\n",
        "        3: 8\n",
        "    }\n",
        "  else:\n",
        "    assert config['dataset_name'] == 'Robot'\n",
        "    config['n_epochs_list'] = [40, 40, 40, 40]\n",
        "\n",
        "    in_size_dict = {\n",
        "        1: (x*2, x*3),\n",
        "        2: (x*4, x*6),\n",
        "        3: (x*5, x*8),\n",
        "        4: (x*7, x*10)\n",
        "    }\n",
        "    bs_dict = {\n",
        "        1: 64,\n",
        "        2: 16,\n",
        "        3: 8,\n",
        "        4: 4\n",
        "    }\n",
        "    if train_step != len(config['n_epochs_list']):\n",
        "      config['learning_rate'] = 5*1e-4\n",
        "    else:\n",
        "      config['learning_rate'] = 1e-4\n",
        "\n",
        "  config['in_size'] = in_size_dict[train_step]\n",
        "  config['batch_size'] = bs_dict[train_step]\n",
        "  config['n_epochs'] = config['n_epochs_list'][train_step-1]\n",
        "  assert config['n_epochs'] % config['save_step'] == 0\n",
        "  config['start_epoch'] = (sum(config['n_epochs_list'][:train_step-1]) + 1) if train_step > 1 else 1\n",
        "\n",
        "  # add LocationAwareConv only at the last step\n",
        "  config['loc_aware'] = (train_step == len(config['n_epochs_list']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-svaNTqkoix"
      },
      "source": [
        "init_config()\n",
        "if TRAIN_MODE and PROG_IMG_RESIZING:\n",
        "  adjust_config(TRAIN_STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stbrdst2QN_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29034d07-889c-4501-d17a-5187f1bfbe37"
      },
      "source": [
        "root_dir = ''\n",
        "if MOUNT_GDRIVE:\n",
        "  from google.colab import drive\n",
        "  root_dir = '/content/Drive/'\n",
        "  drive.mount(root_dir)\n",
        "  root_dir += 'MyDrive/VideoPredictionProject/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/Drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kbJmfxa2yGw"
      },
      "source": [
        "def create_dir(dir):\n",
        "  os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "def create_dir_and_subdir(dir):\n",
        "  dataset_name = config['dataset_name'].lower()\n",
        "  subdir = os.path.join(dir, dataset_name)\n",
        "  create_dir(dir)\n",
        "  create_dir(subdir)\n",
        "  return subdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qun5MI_t_Mox"
      },
      "source": [
        "save_dir = create_dir_and_subdir(os.path.join(root_dir, 'models'))\n",
        "data_dir = create_dir_and_subdir(os.path.join(root_dir, 'data'))\n",
        "res_dir = create_dir_and_subdir(os.path.join(root_dir, 'results'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt-HSWrnvWs_"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    avDev = torch.device(\"cuda\")\n",
        "else:\n",
        "    avDev = torch.device(\"cpu\")\n",
        "print(avDev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vElTHPR2xFl_"
      },
      "source": [
        "**Network implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWWpk6eRXZ7d"
      },
      "source": [
        "# Location Dependent Convolution\n",
        "# Source: https://github.com/AIS-Bonn/LocDepVideoPrediction/blob/master/vlnOrig.ipynb\n",
        "\n",
        "class LocationAwareConv2d(torch.nn.Conv2d):\n",
        "  def __init__(self, locationAware, gradient, in_size, in_channels, out_channels,\n",
        "               kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "    super().__init__(in_channels, out_channels, kernel_size, stride=stride,\n",
        "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "    h, w = in_size\n",
        "    if locationAware:\n",
        "      self.locationBias=torch.nn.Parameter(torch.zeros(h, w, 3))\n",
        "      self.locationEncode=torch.autograd.Variable(torch.ones(h, w, 3))\n",
        "      if gradient:\n",
        "        for i in range(h):\n",
        "          self.locationEncode[i,:,1] = self.locationEncode[:,i,0] = i/float(h-1)\n",
        "        \n",
        "    self.up = torch.nn.Upsample(size=(h, w), mode='bilinear', align_corners=False)\n",
        "    self.h = h\n",
        "    self.w = w\n",
        "    self.locationAware=locationAware\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    if self.locationAware:\n",
        "      if self.locationBias.device != inputs.device:\n",
        "        self.locationBias = self.locationBias.to(inputs.device)\n",
        "      if self.locationEncode.device != inputs.device:\n",
        "        self.locationEncode = self.locationEncode.to(inputs.device)\n",
        "      b = self.locationBias*self.locationEncode\n",
        "    convRes = super().forward(inputs)\n",
        "    if convRes.shape[2] != self.h and convRes.shape[3] != self.w:\n",
        "      convRes = self.up(convRes)\n",
        "    if self.locationAware:\n",
        "      return convRes + b[:,:,0] + b[:,:,1] + b[:,:,2]\n",
        "    else:\n",
        "      return convRes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_76Wo6vXVmY"
      },
      "source": [
        "# Convolutional GRU\n",
        "# Source: https://github.com/happyjin/ConvGRU-pytorch/blob/master/convGRU.py\n",
        "\n",
        "class ConvGRUCell(nn.Module):\n",
        "  def __init__(self, in_channels, hid_channels, kernel_size, bias=True):\n",
        "    super(ConvGRUCell, self).__init__()\n",
        "    self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "    self.hid_channels = hid_channels\n",
        "    self.bias = bias\n",
        "    self.h_cur = None\n",
        "  \n",
        "    self.conv_gates = nn.Conv2d(in_channels=in_channels + hid_channels,\n",
        "                                out_channels=2*self.hid_channels,  # for update_gate,reset_gate respectively\n",
        "                                kernel_size=kernel_size,\n",
        "                                padding=self.padding,\n",
        "                                bias=self.bias)\n",
        "\n",
        "    self.conv_can = nn.Conv2d(in_channels=in_channels + hid_channels,\n",
        "                              out_channels=self.hid_channels, # for candidate neural memory\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    assert self.h_cur is not None\n",
        "    self.h_cur = self.h_cur.to(input.device)\n",
        "\n",
        "    combined = torch.cat([input, self.h_cur], dim=1)\n",
        "    combined_conv = self.conv_gates(combined)\n",
        "\n",
        "    gamma, beta = torch.split(combined_conv, self.hid_channels, dim=1)\n",
        "    reset_gate = torch.sigmoid(gamma)\n",
        "    update_gate = torch.sigmoid(beta)\n",
        "        \n",
        "    combined = torch.cat([input, reset_gate*self.h_cur], dim=1)\n",
        "    cc_cnm = self.conv_can(combined)\n",
        "    cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "    self.h_cur = (1 - update_gate) * self.h_cur + update_gate * cnm\n",
        "    return self.h_cur\n",
        "\n",
        "  def init_hidden(self, batch_size, in_size):\n",
        "    self.h_cur = torch.zeros(batch_size, self.hid_channels, in_size[0], in_size[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ghnX5aVWTyh"
      },
      "source": [
        "def Conv1x1(in_channels: int, out_channels: int, stride: int=1):\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True)\n",
        "\n",
        "def Conv3x3(in_channels: int, out_channels: int):\n",
        "  return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
        "  \n",
        "def ConvTrans2x2(in_channels: int, out_channels: int):\n",
        "  return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=True)\n",
        "\n",
        "def ReLU_BN(in_channels: int):\n",
        "  return nn.Sequential(nn.ReLU(), nn.BatchNorm2d(in_channels))\n",
        "\n",
        "def Conv_PixelShuffle_Conv(in_channels: int, out_channels: int):\n",
        "  hid_channels = 1024\n",
        "  upscale_factor = 2\n",
        "  return nn.Sequential(\n",
        "            Conv3x3(in_channels, hid_channels),\n",
        "            nn.PixelShuffle(upscale_factor),\n",
        "            Conv3x3(hid_channels//(upscale_factor**2), out_channels)\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLh7DGzSlQsL"
      },
      "source": [
        "class BridgeBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Recurrent residual block between encoder and decoder layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_size: int, in_channels: int,\n",
        "               hid_channels: int, loc_aware: bool=False):\n",
        "    \"\"\"\n",
        "    param in_size: size of the input image frame.\n",
        "    param in_channels: number of input channels.\n",
        "    param hid_channels: number of hidden channels.\n",
        "    param loc_aware: specifies whether to use location-dependent conv. or not\n",
        "    \"\"\"\n",
        "    super(BridgeBlock, self).__init__()\n",
        "    self._in_size = in_size\n",
        "    self._in_channels = in_channels\n",
        "    self._hid_channels = hid_channels\n",
        "        \n",
        "    if loc_aware:\n",
        "      self.conv1x1 = LocationAwareConv2d(True, True, in_size, in_channels, hid_channels, 1)\n",
        "    else:\n",
        "      self.conv1x1 = Conv1x1(in_channels, hid_channels)\n",
        "\n",
        "    self.conv_gru_1 = ConvGRUCell(in_channels, hid_channels, kernel_size=(3,3))\n",
        "    self.conv_gru_2 = ConvGRUCell(in_channels, hid_channels, kernel_size=(5, 5))\n",
        "    self.conv_gru_3 = ConvGRUCell(in_channels, hid_channels, kernel_size=(7, 7))\n",
        "    self.gru_cells = [self.conv_gru_1, self.conv_gru_2, self.conv_gru_3]\n",
        "    \n",
        "  def forward(self, input: torch.Tensor):\n",
        "    x1 = self.conv1x1(input)\n",
        "    x2 = self.conv_gru_1(input)\n",
        "    x3 = self.conv_gru_2(input)\n",
        "    x4 = self.conv_gru_3(input)\n",
        "    out = torch.cat([x1, x2, x3, x4], dim=1)\n",
        "    return out\n",
        "  \n",
        "  def init_hidden(self, batch_size: int):\n",
        "    \"\"\"\n",
        "    Initialize the hidden states of the convGRU blocks.\n",
        "    \"\"\"\n",
        "    for gru_cell in self.gru_cells:\n",
        "      gru_cell.init_hidden(batch_size, self._in_size)\n",
        "\n",
        "  def insert_loc_dep_conv(self):\n",
        "    \"\"\"\n",
        "    Insert a 1x1 LocationAwareConv2d layer in the block.\n",
        "    \"\"\"\n",
        "    self.conv1x1 = LocationAwareConv2d(True, True, self._in_size, self._in_channels, self._hid_channels, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIsa6DVQ7dOL"
      },
      "source": [
        "class NextFrameNet(ResNet):\n",
        "  \"\"\"\n",
        "  Outputs the next frame conditioned on the previous frames.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_size: Tuple[int, int]=(224, 224),\n",
        "               loc_aware: bool=False, freeze_encoder: bool=True):\n",
        "    \"\"\"\n",
        "    param in_size: input image frame size\n",
        "    param loc_aware: specifies whether to use location-dependent conv. or not\n",
        "    param freeze_encoder: specifies whether to freeze the ResNet-18 layers or not\n",
        "    \"\"\"\n",
        "    super(NextFrameNet, self).__init__(BasicBlock, [2,2,2,2])\n",
        "    \n",
        "    # Encoder: a pretrained Resnet-18 network without the last GAP and FC layers\n",
        "    resnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n",
        "    state_dict = model_zoo.load_url(resnet18_url)\n",
        "    self.load_state_dict(state_dict)\n",
        "    del self.avgpool\n",
        "    del self.fc\n",
        "\n",
        "    if freeze_encoder:\n",
        "      for _, child in self.named_children():\n",
        "        for _, params in child.named_parameters():\n",
        "          params.requires_grad = False\n",
        "\n",
        "    h, w = in_size\n",
        "    in_dim = h_dim = 64\n",
        "    \n",
        "    # Residual recurrent blocks between Encoder and Decoder layers\n",
        "    self.bridge_block1 = BridgeBlock((h//2, w//2), in_dim, h_dim)\n",
        "    self.bridge_block2 = BridgeBlock((h//4, w//4), in_dim, h_dim)\n",
        "    self.bridge_block3 = BridgeBlock((h//8, w//8), in_dim*2, h_dim, loc_aware)\n",
        "    self.bridge_block4 = BridgeBlock((h//16, w//16), in_dim*4, h_dim, loc_aware)\n",
        "    \n",
        "    self.bridge_blocks = [self.bridge_block1, self.bridge_block2,\n",
        "                          self.bridge_block3, self.bridge_block4]\n",
        "    \n",
        "    # Decoder: uses PixelShuffle for upsampling\n",
        "    self.relu_1 = nn.ReLU()\n",
        "    self.pixel_shuffle_block1 = Conv_PixelShuffle_Conv(h_dim*8, h_dim)\n",
        "\n",
        "    self.relu_bn_2 = ReLU_BN(h_dim*5)\n",
        "    self.pixel_shuffle_block2 = Conv_PixelShuffle_Conv(h_dim*5, h_dim)\n",
        "\n",
        "    self.relu_bn_3 = ReLU_BN(h_dim*5)\n",
        "    self.pixel_shuffle_block3 = Conv_PixelShuffle_Conv(h_dim*5, h_dim) \n",
        "\n",
        "    self.relu_bn_4 = ReLU_BN(h_dim*5)\n",
        "    self.pixel_shuffle_block4 = Conv_PixelShuffle_Conv(h_dim*5, h_dim)\n",
        "\n",
        "    self.conv_trans_2x2 = ConvTrans2x2(h_dim*5, 3)\n",
        "\n",
        "  def forward(self, input: torch.Tensor):\n",
        "    x0 = self.conv1(input)\n",
        "    x = self.bn1(x0)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "        \n",
        "    x1 = self.layer1(x)\n",
        "    x2 = self.layer2(x1)\n",
        "    x3 = self.layer3(x2)\n",
        "    x4 = self.layer4(x3)\n",
        "\n",
        "    x0_1 = self.bridge_block1(x0)\n",
        "    x1_1 = self.bridge_block2(x1)\n",
        "    x2_1 = self.bridge_block3(x2)\n",
        "    x3_1 = self.bridge_block4(x3)\n",
        "\n",
        "    x4 = self.relu_1(x4)\n",
        "    x4 = self.pixel_shuffle_block1(x4)\n",
        "\n",
        "    x4_1 = torch.cat([x3_1, x4], dim=1)\n",
        "    x4_1 = self.relu_bn_2(x4_1)\n",
        "    x4_1 = self.pixel_shuffle_block2(x4_1)\n",
        "    \n",
        "    x4_2 = torch.cat([x2_1, x4_1], dim=1)\n",
        "    x4_2 = self.relu_bn_3(x4_2)\n",
        "    x4_2 = self.pixel_shuffle_block3(x4_2)\n",
        "  \n",
        "    x4_3 = torch.cat([x1_1, x4_2], dim=1)\n",
        "    x4_3 = self.relu_bn_4(x4_3)\n",
        "    x4_3 = self.pixel_shuffle_block4(x4_3)\n",
        "\n",
        "    x4_4 = torch.cat([x0_1, x4_3], dim=1)\n",
        "\n",
        "    out = self.conv_trans_2x2(x4_4)\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self, batch_size: int):\n",
        "    \"\"\"\n",
        "    Initialize the hidden states of the convGRU blocks.\n",
        "    \"\"\"\n",
        "    for bridge_block in self.bridge_blocks:\n",
        "      bridge_block.init_hidden(batch_size)\n",
        "\n",
        "  def insert_loc_dep_conv(self):\n",
        "    \"\"\"\n",
        "    Replace simple 1x1 conv. layers by location aware conv.\n",
        "    (after loading pretrained model).\n",
        "    \"\"\"\n",
        "    self.bridge_block3.insert_loc_dep_conv()\n",
        "    self.bridge_block4.insert_loc_dep_conv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvl7jSAZ0c3k"
      },
      "source": [
        "class AutoregressiveModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Takes a sequence of frames and runs through the model.\n",
        "  Each of the next few frames are predicted based on the currently predicted frame.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, in_size: Tuple[int, int]=(224, 224),\n",
        "               loc_aware: bool=False, freeze_encoder: bool=True):\n",
        "    \"\"\"\n",
        "    param in_size: input image frame size\n",
        "    param loc_aware: specifies whether to use location-dependent conv. or not\n",
        "    param freeze_encoder: specifies whether to freeze the ResNet-18 layers or not\n",
        "    \"\"\"\n",
        "    super(AutoregressiveModel, self).__init__()\n",
        "    self.net = NextFrameNet(in_size, loc_aware, freeze_encoder)\n",
        "\n",
        "  def forward(self, input: torch.Tensor):\n",
        "    \"\"\"\n",
        "    :param input: input sequence of frames (BxFxCxHxW).\n",
        "    \"\"\"\n",
        "    self.net.init_hidden(batch_size=input.size(0))\n",
        "\n",
        "    n_frames = input.size(1)\n",
        "    for i in range(n_frames):\n",
        "      x = self.net(input[:,i])\n",
        "    res = [x]\n",
        "\n",
        "    for i in range(1, n_frames):\n",
        "      x = self.net(x)\n",
        "      res.append(x)\n",
        "\n",
        "    out = torch.stack(res, dim=1)\n",
        "    return out\n",
        "\n",
        "  def insert_loc_dep_conv(self):\n",
        "    \"\"\"\n",
        "    Replace simple 1x1 conv. layers by location aware conv.\n",
        "    (after loading pretrained model).\n",
        "    \"\"\"\n",
        "    self.net.insert_loc_dep_conv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGXTLvCxPqu"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXjUQKfuzrm6"
      },
      "source": [
        "class MovingMnist(data.Dataset):\n",
        "  \"\"\"\n",
        "  Custom Dataset class for downloading and working with MovingMnist dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, data_dir: str, download: bool=True, train: bool=True,\n",
        "               train_split: int=9000, transform: Optional[transforms.Compose]=None,\n",
        "               convert_rgb: bool=True, n_frames: int=3):\n",
        "    \"\"\"\n",
        "    param data_dir: directory path to store data files.\n",
        "    param download: specifies whether to download the .gz file or use the existing one.\n",
        "    param train: train or test dataset.\n",
        "    param train_split: number of samples to be used for training.\n",
        "    param transform: transformation to be applied to each frame.\n",
        "    param convert_rgb: specifies whether to convert the frames to RGB by duplicating the channel.\n",
        "    param n_frames: number of input/target frames in each sample.\n",
        "    \"\"\"\n",
        "    self.train = train\n",
        "    self.transform = transform\n",
        "    self.convert_rgb = convert_rgb\n",
        "    self.n_frames = n_frames\n",
        "\n",
        "    npy_file_name = 'MovingMnist.npy'\n",
        "    npy_file = os.path.join(data_dir, npy_file_name)\n",
        "    if download:\n",
        "      url = 'https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz'\n",
        "      gz_file = self.__download(url, data_dir)\n",
        "      # unzip the downloaded .gz data file\n",
        "      with open(npy_file, 'wb') as out_f:\n",
        "        with gzip.GzipFile(gz_file) as zip_f:\n",
        "          out_f.write(zip_f.read())\n",
        "    else:\n",
        "      assert os.path.isfile(npy_file), f'{npy_file} data file not found!'\n",
        "    \n",
        "    data_npy = np.load(npy_file)\n",
        "    data_npy = data_npy.swapaxes(0, 1)\n",
        "\n",
        "    assert train_split != 0\n",
        "    if self.train:\n",
        "      self.train_data = torch.from_numpy(data_npy[:train_split])\n",
        "    else:\n",
        "      self.test_data = torch.from_numpy(data_npy[train_split:])\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the dataset length (train or test).\n",
        "    \"\"\"\n",
        "    if self.train:\n",
        "      return len(self.train_data)\n",
        "    else:\n",
        "      return len(self.test_data)\n",
        "\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Returns a tuple of input and traget frame sequences at the given sample index.\n",
        "    \"\"\"\n",
        "    n_frames = self.n_frames\n",
        "    if self.train:\n",
        "      fidx = random.randint(0, self.train_data.size(1)-2*n_frames) if RANDOMIZE else 0\n",
        "      input, target = self.train_data[index, fidx:fidx+n_frames], self.train_data[index, fidx+n_frames:fidx+2*n_frames]\n",
        "    else:\n",
        "      input, target = self.test_data[index, :n_frames], self.test_data[index, n_frames:2*n_frames]\n",
        "\n",
        "    input = self.__transform(input)\n",
        "    target = self.__transform(target)\n",
        "    return input, target\n",
        "\n",
        "  def __transform(self, data: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Transforms the given sequence of frames.\n",
        "    \"\"\"\n",
        "    frame_seq = []\n",
        "    for i in range(data.size(0)):\n",
        "      frame = data[i].numpy()\n",
        "      frame = Image.fromarray(frame)\n",
        "      if self.convert_rgb:\n",
        "        frame = frame.convert('RGB')\n",
        "      if self.transform is not None:\n",
        "        frame = self.transform(frame)\n",
        "      else:\n",
        "        frame = transforms.ToTensor()(frame)\n",
        "      frame_seq.append(frame)\n",
        "    frame_seq = torch.stack(frame_seq, dim=0)\n",
        "    return frame_seq\n",
        "\n",
        "  def __download(self, url: str, data_dir: str) -> os.path:\n",
        "    \"\"\"\n",
        "    Downloads the data file from the specified url and stores in the data_dir.\n",
        "    \"\"\"\n",
        "    filename = url.rpartition('/')[2]\n",
        "    file_path = os.path.join(data_dir, filename)\n",
        "    if os.path.exists(file_path):\n",
        "      return file_path\n",
        "    create_dir(data_dir)\n",
        "    data = urllib.request.urlopen(url)\n",
        "    with open(file_path, 'wb') as f:\n",
        "      f.write(data.read())\n",
        "    return file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dJq2ht_TTA"
      },
      "source": [
        "class UnNormalize(object):\n",
        "  def __init__(self, mean, std):\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(self, tensor):\n",
        "    for i in range(tensor.size(0)):\n",
        "      for t, m, s in zip(tensor[i], self.mean, self.std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "transform = transforms.Compose([\n",
        "                                  transforms.Resize(config['in_size']),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize(mean, std)\n",
        "                               ])\n",
        "unorm = UnNormalize(mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb-B75InAdvr"
      },
      "source": [
        "def get_datasets(dataset_name):\n",
        "  train_split = config['train_split']\n",
        "  if dataset_name == 'Mnist':    \n",
        "    train_data = MovingMnist(data_dir, download=True, train=True, train_split=train_split, transform=transform,\n",
        "                             convert_rgb=True, n_frames=config['n_frames'])\n",
        "    test_data = MovingMnist(data_dir, download=False, train=False, train_split=train_split, transform=transform,\n",
        "                            convert_rgb=True, n_frames=config['n_frames'])\n",
        "  else:\n",
        "    assert dataset_name == 'Robot'\n",
        "    train_skip_step = 7 # subsample to get around 10700 samples\n",
        "    train_data = VideoFrameDataset(train_urls, train_ivals, data_dir=data_dir, train=True,\n",
        "                                   transform=transform, sample_step=2, skip_step=train_skip_step, n_frames=config['n_frames'])\n",
        "    \n",
        "    test_skip_step = 3 # subsample to get around 1700 samples\n",
        "    test_data = VideoFrameDataset(test_urls, test_ivals, data_dir=data_dir, train=False,\n",
        "                                  transform=transform, sample_step=2, skip_step=test_skip_step, n_frames=config['n_frames'])\n",
        "  return train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5R4TM6RvKI_"
      },
      "source": [
        "start_time = time.time()\n",
        "train_data, test_data = get_datasets(config['dataset_name'])\n",
        "end_time = time.time()\n",
        "print('Time elapsed: {} mins'.format(round((end_time-start_time)/60, 1)))\n",
        "\n",
        "dataset_size = len(train_data)\n",
        "print('Train dataset size:', dataset_size)\n",
        "print('Test dataset size:', len(test_data))\n",
        "\n",
        "trainset_size = int(dataset_size*0.8)\n",
        "valset_size = dataset_size - trainset_size\n",
        "trainset, valset = random_split(train_data, [trainset_size, valset_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=config['batch_size'],\n",
        "                                           shuffle=True, num_workers=config['n_workers'])\n",
        "val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=config['batch_size'],\n",
        "                                         shuffle=False, num_workers=config['n_workers'])\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_loader,\n",
        "    'val': val_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8sE7X5Ax8Cr"
      },
      "source": [
        "**Loss functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwYbxARcb30h"
      },
      "source": [
        "# Source: https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/ssim.html\n",
        "\n",
        "def ssim(img1: torch.Tensor, img2: torch.Tensor, window_size: int,\n",
        "         max_val: float = 1.0, eps: float = 1e-12) -> torch.Tensor:\n",
        "\n",
        "    if not isinstance(img1, torch.Tensor):\n",
        "        raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(img1)))\n",
        "\n",
        "    if not isinstance(img2, torch.Tensor):\n",
        "        raise TypeError(\"Input img2 type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(img2)))\n",
        "\n",
        "    if not isinstance(max_val, float):\n",
        "        raise TypeError(f\"Input max_val type is not a float. Got {type(max_val)}\")\n",
        "\n",
        "    if not len(img1.shape) == 4:\n",
        "        raise ValueError(\"Invalid img1 shape, we expect BxCxHxW. Got: {}\"\n",
        "                         .format(img1.shape))\n",
        "\n",
        "    if not len(img2.shape) == 4:\n",
        "        raise ValueError(\"Invalid img2 shape, we expect BxCxHxW. Got: {}\"\n",
        "                         .format(img2.shape))\n",
        "\n",
        "    if not img1.shape == img2.shape:\n",
        "        raise ValueError(\"img1 and img2 shapes must be the same. Got: {} and {}\"\n",
        "                         .format(img1.shape, img2.shape))\n",
        "\n",
        "    # prepare kernel\n",
        "    kernel: torch.Tensor = (\n",
        "        get_gaussian_kernel2d((window_size, window_size), (1.5, 1.5)).unsqueeze(0)\n",
        "    )\n",
        "\n",
        "    # compute coefficients\n",
        "    C1: float = (0.01 * max_val) ** 2\n",
        "    C2: float = (0.03 * max_val) ** 2\n",
        "\n",
        "    # compute local mean per channel\n",
        "    mu1: torch.Tensor = filter2D(img1, kernel)\n",
        "    mu2: torch.Tensor = filter2D(img2, kernel)\n",
        "\n",
        "    mu1_sq = mu1 ** 2\n",
        "    mu2_sq = mu2 ** 2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    # compute local sigma per channel\n",
        "    sigma1_sq = filter2D(img1 ** 2, kernel) - mu1_sq\n",
        "    sigma2_sq = filter2D(img2 ** 2, kernel) - mu2_sq\n",
        "    sigma12 = filter2D(img1 * img2, kernel) - mu1_mu2\n",
        "\n",
        "    # compute the similarity index map\n",
        "    num: torch.Tensor = (2. * mu1_mu2 + C1) * (2. * sigma12 + C2)\n",
        "    den: torch.Tensor = (\n",
        "        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
        "    )\n",
        "\n",
        "    return num / (den + eps)\n",
        "\n",
        "def ssim_loss(img1: torch.Tensor, img2: torch.Tensor, window_size: int,\n",
        "              max_val: float = 1.0, eps: float = 1e-12, reduction: str = 'mean') -> torch.Tensor:\n",
        "    \n",
        "    # compute the ssim map\n",
        "    ssim_map: torch.Tensor = ssim(img1, img2, window_size, max_val, eps)\n",
        "\n",
        "    # compute and reduce the loss\n",
        "    loss = torch.clamp((1. - ssim_map) / 2, min=0, max=1)\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = torch.mean(loss)\n",
        "    elif reduction == \"sum\":\n",
        "        loss = torch.sum(loss)\n",
        "    elif reduction == \"none\":\n",
        "        pass\n",
        "    return loss\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def __init__(self, window_size: int, max_val: float = 1.0,\n",
        "                 eps: float = 1e-12, reduction: str = 'mean') -> None:\n",
        "        super(SSIMLoss, self).__init__()\n",
        "        self.window_size: int = window_size\n",
        "        self.max_val: float = max_val\n",
        "        self.eps: float = eps\n",
        "        self.reduction: str = reduction\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
        "        return ssim_loss(img1, img2, self.window_size, self.max_val, self.eps, self.reduction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7zyXo0MUD-H"
      },
      "source": [
        "SSIMloss = SSIMLoss(window_size=config['ssim_win_size'])\n",
        "MSEloss = nn.MSELoss()\n",
        "\n",
        "def MSE_DSSIM_Loss(pred, target):\n",
        "  \"\"\"\n",
        "  param pred: predicted sequence of frames of size BxFxCxHxW\n",
        "  param target: target sequence of frames of size BxFxCxHxW\n",
        "  \"\"\"\n",
        "  alpha = config['mse_gain']\n",
        "  mse_loss = 0.0\n",
        "  dssim_loss = 0.0\n",
        "  for i in range(pred.size(1)):\n",
        "    mse_loss += MSEloss(pred[:,i], target[:,i])\n",
        "    dssim_loss += SSIMloss(pred[:,i], target[:,i])\n",
        "  mse_loss /= pred.size(1)\n",
        "  dssim_loss /= pred.size(1)\n",
        "  loss = alpha * mse_loss + (1 - alpha) * dssim_loss\n",
        "  return loss, mse_loss, dssim_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HdguCgGxbC4"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ9rxvO0RPDC"
      },
      "source": [
        "def save_checkpoint(model, epoch):\n",
        "  torch.save(model.state_dict(), f'{save_dir}/model_{epoch}.pth')\n",
        "  \n",
        "def load_from_checkpoint(model, epoch):\n",
        "  model.load_state_dict(torch.load(f'{save_dir}/model_{epoch}.pth', map_location='cpu'))\n",
        "  model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2P1YGYEjUIG"
      },
      "source": [
        "def plot_losses(train_losses, val_losses, start_epoch, end_epoch):\n",
        "  x = np.linspace(start_epoch, end_epoch, end_epoch - start_epoch + 1)\n",
        "  phase_names = ['train', 'val']\n",
        "  losses = [train_losses, val_losses]\n",
        "  for phase_name, loss in zip(phase_names, losses):\n",
        "    plt.plot(x, loss, label=phase_name)\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.xaxis.get_major_locator().set_params(integer=True)\n",
        "  plt.xlabel(\"#epochs\")\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  \n",
        "  plt.savefig(f'{res_dir}/losses_{end_epoch}.png')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "def plot_combined_losses():\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for i in range(1, len(config['n_epochs_list'])+1):\n",
        "    start_epoch = sum(config['n_epochs_list'][:i-1]) + 1 if i > 1 else 1\n",
        "    end_epoch = sum(config['n_epochs_list'][:i])\n",
        "    train_losses.append(np.load(f'{res_dir}/train_{start_epoch}_{end_epoch}.npy'))\n",
        "    val_losses.append(np.load(f'{res_dir}/val_{start_epoch}_{end_epoch}.npy'))\n",
        "    \n",
        "  train_losses = np.concatenate(train_losses)\n",
        "  val_losses = np.concatenate(val_losses)\n",
        "  plot_losses(train_losses, val_losses, 1, end_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pDmmWgdaZUh"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, dataloaders, start_epoch):\n",
        "  n_epochs = config['n_epochs'] + start_epoch - 1\n",
        "  save_step = config['save_step']\n",
        "  n_frames = config['n_frames']\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for epoch in range(start_epoch, n_epochs+1):\n",
        "    start_time = time.time()\n",
        "    print('Epoch {}/{}'.format(epoch, n_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()\n",
        "        RANDOMIZE = True\n",
        "      else:\n",
        "        model.eval()\n",
        "        RANDOMIZE = False\n",
        "          \n",
        "      running_loss = running_mse = running_dssim = 0.0\n",
        "      dataset_size = 0\n",
        "      dataloader = dataloaders[phase]\n",
        "      for batch1, batch2 in dataloader:\n",
        "        batch1 = batch1.to(avDev)\n",
        "        batch2 = batch2.to(avDev)\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          preds = model(batch1.float())\n",
        "          loss, mse, dssim = criterion(preds.float(), batch2.float())\n",
        "\n",
        "          if phase == 'train':\n",
        "            loss.backward()                      \n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch1.size(0)\n",
        "        running_mse += mse.item() * batch1.size(0)\n",
        "        running_dssim += dssim.item() * batch1.size(0)\n",
        "        dataset_size += batch1.size(0)\n",
        "\n",
        "      epoch_loss = running_loss/dataset_size\n",
        "      print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
        "      print('MSE : {:.4f}'.format(running_mse/dataset_size))\n",
        "      print('DSSIM: {:.4f}'.format(running_dssim/dataset_size))\n",
        "            \n",
        "      if phase == 'train':\n",
        "        train_losses.append(epoch_loss)\n",
        "      else:\n",
        "        val_losses.append(epoch_loss)\n",
        "    \n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "              \n",
        "    if epoch % save_step == 0:\n",
        "      save_checkpoint(model, epoch)\n",
        "      plot_losses(train_losses, val_losses, start_epoch, epoch)\n",
        "    end_time = time.time()\n",
        "    print('Time elapsed: {} mins'.format(round((end_time-start_time)/60, 1)))\n",
        "  return train_losses, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhbZnyy9kILy"
      },
      "source": [
        "# create/load the model\n",
        "\n",
        "load_loc_aware = config['loc_aware']\n",
        "in_size = config['in_size']\n",
        "  \n",
        "if TRAIN_MODE:\n",
        "  load_epoch = config['start_epoch'] - 1\n",
        "  if PROG_IMG_RESIZING:\n",
        "    load_loc_aware = False\n",
        "else:\n",
        "  load_epoch = TEST_EPOCH\n",
        "\n",
        "model = AutoregressiveModel(in_size, load_loc_aware, freeze_encoder=False)\n",
        "if load_epoch > 0:\n",
        "  state_dict = torch.load(f'{save_dir}/model_{load_epoch}.pth', map_location='cpu')\n",
        "  load_from_checkpoint(model, load_epoch)\n",
        "  if TRAIN_MODE and PROG_IMG_RESIZING and config['loc_aware']:\n",
        "    model.insert_loc_dep_conv()\n",
        "model = model.to(avDev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mxyrdr6Dmkd"
      },
      "source": [
        "if TRAIN_MODE:\n",
        "  Optimizer = config['optimizer']\n",
        "  optimizer = Optimizer(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "  train_losses, val_losses = train_model(model, MSE_DSSIM_Loss, optimizer, scheduler=None,\n",
        "                                         dataloaders=dataloaders, start_epoch=config['start_epoch'])\n",
        "  # plot final lurning curves\n",
        "  start_epoch = config['start_epoch']\n",
        "  end_epoch = config['n_epochs'] + start_epoch - 1\n",
        "  \n",
        "  plot_losses(train_losses, val_losses, start_epoch, end_epoch)\n",
        "\n",
        "  np.save(f'{res_dir}/train_{start_epoch}_{end_epoch}', train_losses)\n",
        "  np.save(f'{res_dir}/val_{start_epoch}_{end_epoch}', val_losses)\n",
        "\n",
        "  if PROG_IMG_RESIZING and TRAIN_STEP == len(config['n_epochs_list']):\n",
        "    plot_combined_losses()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT972DiuxhX4"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv6ei6I8oMcj"
      },
      "source": [
        "# Helper functions for reporting the performance metrics\n",
        "@torch.no_grad()\n",
        "def test_metrics(model, data_loader):\n",
        "  model.eval()\n",
        "  running_mse = running_dssim = 0.0\n",
        "  dataset_size = 0.0\n",
        "  for input, target in data_loader:\n",
        "    input = input.to(avDev)\n",
        "    target = target.to(avDev)    \n",
        "    preds = model(input.float())\n",
        "    _, mse, dssim = MSE_DSSIM_Loss(preds, target)\n",
        "    running_mse += mse.item() * input.size(0)\n",
        "    running_dssim += dssim.item() * input.size(0)\n",
        "    dataset_size += input.size(0)\n",
        "\n",
        "  mse = float(running_mse/dataset_size)\n",
        "  dssim = float(running_dssim/dataset_size)\n",
        "  return mse, dssim\n",
        "\n",
        "def save_test_metrics(model, data_loader, dataset_name, fname, mode='w'):\n",
        "  mse, dssim = test_metrics(model, data_loader)\n",
        "  mse = round(mse, 4)\n",
        "  dssim = round(dssim, 4)\n",
        "\n",
        "  f_test_metrics = open(fname, mode)\n",
        "  f_test_metrics.write(f'MSE on {dataset_name} data:{mse}\\n')\n",
        "  f_test_metrics.write(f'DSSIM on {dataset_name} data:{dssim}\\n\\n')\n",
        "  f_test_metrics.close()\n",
        "\n",
        "  print(f'MSE on {dataset_name} data:', mse)\n",
        "  print(f'DSSIM on {dataset_name} data:', dssim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIS656p5t3zU"
      },
      "source": [
        "# Helper functions for plotting the predicted frames\n",
        "def save_animation(seq1, seq2, fname='anim.gif'):\n",
        "  assert seq1.size() == seq2.size(), \"Input and target sizes must match!\"\n",
        "  n_frames = seq1.size(0)\n",
        "  n_channels = seq1.size(1)\n",
        "  to_pil_img = transforms.ToPILImage()\n",
        "  fig = plt.figure()\n",
        "  ims = []\n",
        "\n",
        "  for i in range(n_frames):\n",
        "    frame = seq1[i].cpu()\n",
        "    plt.axis('off')\n",
        "    frame = to_pil_img(frame)\n",
        "    if n_channels != 3:\n",
        "      frame = frame.convert('RGB')\n",
        "    frame_with_border = ImageOps.expand(frame, border=5, fill='red')\n",
        "    im = plt.imshow(frame_with_border)\n",
        "    ims.append([im])\n",
        "\n",
        "  for i in range(n_frames):\n",
        "    frame = seq2[i].cpu()\n",
        "    plt.axis('off')\n",
        "    frame = to_pil_img(frame)\n",
        "    if n_channels != 3:\n",
        "      frame = frame.convert('RGB')\n",
        "    frame_with_border = ImageOps.expand(frame, border=5, fill='green')\n",
        "    im = plt.imshow(frame_with_border)\n",
        "    ims.append([im])\n",
        "\n",
        "  anim = animation.ArtistAnimation(fig, ims, interval=300, repeat_delay=1000)\n",
        "  anim.save(fname, writer=animation.PillowWriter(fps=60))\n",
        "\n",
        "\n",
        "def save_grid(frame_list, fname):\n",
        "  to_pil_img = transforms.ToPILImage()\n",
        "  n_frames = len(frame_list)\n",
        "  fig_h = 10\n",
        "  fig_w = n_frames * fig_h * (frame_list[0].shape[2] / frame_list[0].shape[1])\n",
        "  fig = plt.figure(figsize=(fig_w, fig_h))\n",
        "  grid = ImageGrid(fig, 111, nrows_ncols=(1, n_frames), axes_pad=0.1)\n",
        "  for ax, img in zip(grid, frame_list):\n",
        "    ax.axis('off')\n",
        "    ax.imshow(to_pil_img(img))\n",
        "  plt.savefig(fname)\n",
        "  plt.show()\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_predictions(model, input, target, dataset_name, id):\n",
        "  model.eval()\n",
        "  preds = model(input.float())\n",
        "  clip1 = input[0].clone()\n",
        "  clip2 = target[0].clone()\n",
        "  pred2 = preds[0]\n",
        "  clip1 = unorm(clip1)\n",
        "  clip2 = unorm(clip2)\n",
        "  pred2 = unorm(pred2)\n",
        "  # post-processing\n",
        "  pred2 = pred2.clamp(0, 1)\n",
        "\n",
        "  save_animation(clip1, clip2, fname=f'{res_dir}/original_{dataset_name}_{id}.gif')\n",
        "  save_animation(clip1, pred2, fname=f'{res_dir}/predicted_{dataset_name}_{id}.gif')\n",
        "\n",
        "  save_grid(torch.cat([clip1.cpu(), clip2.cpu()], dim=0), fname=f'{res_dir}/original_{dataset_name}_{id}.png')\n",
        "  save_grid(torch.cat([clip1.cpu(), pred2.cpu()], dim=0), fname=f'{res_dir}/predicted_{dataset_name}_{id}.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD_p9tsW7s6k"
      },
      "source": [
        "# save generations on train-data\n",
        "RANDOMIZE = False\n",
        "f_test_metrics = f'{res_dir}/test_metrics.txt'\n",
        "save_test_metrics(model, train_loader, 'train', fname=f_test_metrics, mode='w')\n",
        "save_test_metrics(model, val_loader, 'val', fname=f_test_metrics, mode='a')\n",
        "\n",
        "batch1, batch2 = next(iter(train_loader))\n",
        "batch1 = batch1.to(avDev)\n",
        "batch2 = batch2.to(avDev)\n",
        "for i in range(min(10, batch1.size(0))):\n",
        "  save_predictions(model, batch1[i:i+1], batch2[i:i+1], 'train', i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0cCUrdjmta"
      },
      "source": [
        "# save generations on test-data\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=config['batch_size'],\n",
        "                                          shuffle=True, num_workers=config['n_workers'])\n",
        "save_test_metrics(model, test_loader, 'test', fname=f_test_metrics, mode='a')\n",
        "\n",
        "batch1, batch2 = next(iter(test_loader))\n",
        "batch1 = batch1.to(avDev)\n",
        "batch2 = batch2.to(avDev)\n",
        "for i in range(min(10, batch1.size(0))):\n",
        "  save_predictions(model, batch1[i:i+1], batch2[i:i+1], 'test', i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}